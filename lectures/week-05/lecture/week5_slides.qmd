---
title: "Introduction to Linear Regression"
subtitle: "Week 5: MUSA 5080"
author: "Dr. Elizabeth Delmelle"
date: "October 6, 2025"
format: 
  revealjs:
    theme: simple
    slide-number: true
    chalkboard: true
    code-line-numbers: true
    incremental: false
    smaller: true
---

```{r}
#| include: false
options(scipen = 999)
library(tidyverse)
library(tidycensus)
library(broom)
library(scales)

census_api_key("42bf8a20a3df1def380f330cf7edad0dd5842ce6")
```

# Opening Question

**Scenario:** You're advising a state agency on resource allocation.

Some counties have sparse data. Can you **predict** their median income using data from counties with better measurements?

**Broader question:** How do we make informed predictions when we don't have complete information?

------------------------------------------------------------------------

# Today's Roadmap

1.  **The Statistical Learning Framework:** What are we actually doing?
2.  **Two goals:** Understanding relationships vs Making predictions
3.  **Building your first model** with PA census data
4.  **Model evaluation:** How do we know if it's any good?
5.  **Checking assumptions:** When can we trust the model?
6.  **Improving predictions:** Transformations, multiple variables

------------------------------------------------------------------------

# Part 1: The Statistical Learning Framework

## The General Problem

We observe data: counties, income, population, education, etc.

We believe there's some **relationship** between these variables.

**Statistical learning** = a set of approaches for estimating that relationship

```{r}
library(tidyverse)

# Generate data following Y = f(X) + ε
set.seed(789)
n <- 50
x <- seq(0, 10, length.out = n)

# True function f(X) - let's make it slightly curved
f_x <- 5 + 2*x - 0.1*x^2

# Add random error ε
epsilon <- rnorm(n, 0, 2)
y <- f_x + epsilon

# Create data frame
data <- data.frame(x = x, y = y, f_x = f_x, epsilon = epsilon)

# Highlight a few specific points to show the decomposition
highlight_points <- c(10, 25, 40)
data$highlight <- 1:n %in% highlight_points

# Create the visualization
ggplot(data, aes(x = x)) +
  # The true function f(X)
  geom_line(aes(y = f_x), color = "#2C3E50", linewidth = 1.5) +
  
  # The observed data points Y
  geom_point(aes(y = y, size = highlight, alpha = highlight), 
             color = "#E74C3C", show.legend = FALSE) +
  
  # Show the error ε for highlighted points
  geom_segment(data = data[data$highlight, ],
               aes(x = x, xend = x, y = f_x, yend = y),
               color = "#9B59B6", linewidth = 1, 
               arrow = arrow(length = unit(0.2, "cm"), ends = "both")) +
  
  # Annotations
  annotate("text", x = 2, y = 22, 
           label = "Y = f(X) + ε", 
           size = 6, fontface = "bold", hjust = 0) +
  
  annotate("text", x = 8, y = 18, 
           label = "f(X)\n(systematic)", 
           size = 5, color = "#2C3E50", hjust = 0) +
  
  annotate("text", x = 5.5, y = 10, 
           label = "ε\n(random error)", 
           size = 4, color = "#9B59B6") +
  
  annotate("point", x = 1, y = 8, size = 4, color = "#E74C3C") +
  annotate("text", x = 1.5, y = 8, 
           label = "Observed Y", 
           size = 4, color = "#E74C3C", hjust = 0) +
  
  scale_size_manual(values = c(2, 4)) +
  scale_alpha_manual(values = c(0.6, 1)) +
  
  labs(x = "X (predictors)", 
       y = "Y (outcome)",
       title = "The Statistical Learning Framework",
       subtitle = "Our goal: Estimate f(X) from observed data") +
  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    panel.grid.minor = element_blank()
  )

# Save
ggsave("images/statistical_learning_framework.png", 
       width = 10, height = 6, dpi = 300)
```

![](images/statistical_learning_framework.png){width="80%"}

------------------------------------------------------------------------

## Formalizing the Relationship

For any quantitative response Y and predictors X₁, X₂, ... Xₚ:

$$Y = f(X) + \epsilon$$

Where:

-   **f** = the systematic information X provides about Y
-   **ε** = random error (irreducible)

------------------------------------------------------------------------

## What is f?

**f represents the true relationship** between predictors and outcome

-   It's **fixed** but **unknown**
-   It's what we're trying to estimate
-   Different X values produce different Y values through f

**Example:**

-   Y = median income
-   X = population, education, poverty rate
-   f = the way these factors systematically relate to income

------------------------------------------------------------------------

## Why Estimate f?

Two main reasons:

**1. Prediction**

-   Estimate Y for new observations
-   Don't necessarily care about the exact form of f
-   Focus: accuracy of predictions

**2. Inference**

-   Understand how X affects Y
-   Which predictors matter?
-   What's the nature of the relationship?
-   Focus: interpreting the model

------------------------------------------------------------------------

## How Do We Estimate f?

**Two broad approaches:**

**Parametric Methods**

-   Make an assumption about the functional form (e.g., linear)
-   Reduces problem to estimating a few parameters
-   Easier to interpret
-   **This is what we'll focus on**

**Non-Parametric Methods**

-   Don't assume a specific form
-   More flexible
-   Require more data
-   Harder to interpret

```{r}
#| echo: false
#| fig-width: 12
#| fig-height: 5
# Generate simulated data with a non-linear pattern
set.seed(456)
n <- 100
x <- seq(0, 10, length.out = n)
# True relationship is quadratic with noise
y_true <- 2 + 3*x - 0.3*x^2
y <- y_true + rnorm(n, 0, 2)

sim_data <- data.frame(x = x, y = y, y_true = y_true)

# Parametric approach: assumes linear form
parametric_plot <- ggplot(sim_data, aes(x, y)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_line(aes(y = y_true), color = "red", linetype = "dashed", linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "blue", linewidth = 1.2) +
  labs(
    title = "Parametric Approach",
    subtitle = "Assumes f is linear: Y = β₀ + β₁X",
    x = "X", y = "Y"
  ) +
  annotate("text", x = 2, y = 12, label = "Assumed linear form", 
           color = "blue", size = 4, hjust = 0) +
  annotate("text", x = 2, y = 10, label = "True relationship", 
           color = "red", size = 4, hjust = 0) +
  theme_minimal(base_size = 14)

# Non-parametric approach: lets data determine the shape
nonparametric_plot <- ggplot(sim_data, aes(x, y)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_line(aes(y = y_true), color = "red", linetype = "dashed", linewidth = 1) +
  geom_smooth(method = "loess", se = FALSE, color = "darkgreen", linewidth = 1.2) +
  labs(
    title = "Non-Parametric Approach",
    subtitle = "No assumed form - let data determine shape",
    x = "X", y = "Y"
  ) +
  annotate("text", x = 2, y = 12, label = "Data-driven fit", 
           color = "darkgreen", size = 4, hjust = 0) +
  annotate("text", x = 2, y = 10, label = "True relationship", 
           color = "red", size = 4, hjust = 0) +
  theme_minimal(base_size = 14)

# Side-by-side comparison
library(patchwork)
parametric_plot | nonparametric_plot

# Save if needed
ggsave("parametric_vs_nonparametric.png", width = 12, height = 5, dpi = 300)
```

------------------------------------------------------------------------

## Parametric vs. Non-Parametric

![](images/parametric_vs_nonparametric.png)

**Key difference:**

-   **Parametric (blue):** We assume f is linear, then estimate β₀ and β₁
-   **Non-parametric (green):** We let the data determine the shape of f

::: callout-note
## What about deep learning?

Neural networks are technically parametric (millions of parameters!), but achieve flexibility through parameter quantity rather than assuming a rigid form. We won't cover them in this course, but they follow the same Y = f(X) + ε framework.
:::

------------------------------------------------------------------------

## Parametric Approach: Linear Regression

**The assumption:** Relationship between X and Y is linear

$$Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$$

**The task:** Estimate the β coefficients using our sample data

**The method:** Ordinary Least Squares (OLS)

------------------------------------------------------------------------

## Why Linear Regression?

**Advantages:**

-   Simple and interpretable
-   Well-understood properties
-   Works remarkably well for many problems
-   Foundation for more complex methods

**Limitations:**

-   Assumes linearity (we'll test this)
-   Sensitive to outliers
-   Makes several assumptions (we'll check these)

------------------------------------------------------------------------

# Part 2: Two Different Goals

## Prediction vs Inference

The **same model** serves different purposes:

::::: columns
::: {.column width="50%"}
**Inference**

-   "Does education affect income?"
-   Focus on coefficients
-   Statistical significance matters
-   Understand mechanisms
:::

::: {.column width="50%"}
**Prediction**

-   "What's County Y's income?"
-   Focus on accuracy
-   Prediction intervals matter
-   Don't need to understand why
:::
:::::

**Today:** We'll do both, but emphasize prediction

------------------------------------------------------------------------

## Example: Prediction

**Government use case:**

Census misses people in hard-to-count areas. Can we predict:

-   Income for areas with poor survey response?
-   Population for planning purposes?
-   Resource needs based on demographics?

**The model doesn't explain WHY** these relationships exist, but if predictions are accurate, they're useful for policy

------------------------------------------------------------------------

## Example: Inference

**Research use case:**

Understanding gentrification:

-   Which neighborhood characteristics explain income change?
-   How much does education matter vs. proximity to downtown?
-   Are policy interventions associated with outcomes?

**Here we care about the coefficients** and what they tell us about mechanisms

------------------------------------------------------------------------

## Connection to Week 2: Algorithmic Bias

Remember the healthcare algorithm that discriminated?

**The model:** Predicted healthcare needs using costs as proxy

**Technically:** Probably had good R², low prediction error (good "fit")

**Ethically:** Learned and amplified existing discrimination

::: callout-important
## Critical Point

A model can be statistically "good" while being ethically terrible for decision-making.
:::

------------------------------------------------------------------------

# Part 3: Building Your First Model

## Start with Data and Visualization

Let's apply these concepts to PA counties:

```{r}
#| echo: false
#| eval: true
#| code-line-numbers: "|2-4|6-14|16-20"
# Fetch PA county data directly from Census API
pa_data <- get_acs(
  geography = "county",
  state = "PA",
  variables = c(
    total_pop = "B01003_001",
    median_income = "B19013_001"
  ),
  year = 2022,
  output = "wide"
)

# Visualize the relationship
ggplot(pa_data, aes(x = total_popE, y = median_incomeE)) +
  geom_point(alpha = 0.6, size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "steelblue") +
  labs(
    title = "Population vs Median Income in PA Counties",
    x = "Total Population", 
    y = "Median Household Income"
  ) +
  scale_x_continuous(labels = comma) +
  scale_y_continuous(labels = dollar) +
  theme_minimal()
```

------------------------------------------------------------------------

## What Do We See?

**Before fitting any model, discuss the visualization:**

-   Generally positive relationship
-   Considerable scatter (not deterministic)
-   Most counties are small (clustered left)
-   One large county with surprisingly low income
-   Wider confidence band at higher populations

**Question:** What does this tell us about f(X)?

------------------------------------------------------------------------

## Fit the Model

```{r}
#| echo: true
#| eval: true
model1 <- lm(median_incomeE ~ total_popE, data = pa_data)
summary(model1)
```

------------------------------------------------------------------------

## Interpreting Coefficients

**Intercept (β₀) = \$62,855**

-   Expected income when population = 0
-   Not usually meaningful in practice

**Slope (β₁) = \$0.02**

-   For each additional person, income increases by \$0.02
-   **More useful:** For every 1,000 people, income increases by \~\$20

**Is this relationship real?**

-   p-value \< 0.001 → Very unlikely to see this if true β₁ = 0
-   We can reject the null hypothesis

------------------------------------------------------------------------

## The "Holy Grail" Concept

::::: columns
::: {.column width="45%"}
Our estimates are just that: **estimates** of the true (unknown) parameters

**Key insight:**

-   Red line = true relationship (unknowable)
-   Blue line = our estimate from this sample
-   Different samples → slightly different blue lines
-   Standard errors quantify this uncertainty
:::

::: {.column width="55%"}
![](images/population_vs_sample_regression.png)
:::
:::::

------------------------------------------------------------------------

## Statistical Significance

**The logic:**

1.  **Null hypothesis (H₀):** β₁ = 0 (no relationship)
2.  **Our estimate:** β₁ = 0.02
3.  **Question:** Could we get 0.02 just by chance if H₀ is true?

**t-statistic:** How many standard errors away from 0?

-   Bigger \|t\| = more confidence the relationship is real

**p-value:** Probability of seeing our estimate if H₀ is true

-   Small p → reject H₀, conclude relationship exists

------------------------------------------------------------------------

# Part 4: Model Evaluation

## How Good is This Model?

**Two key questions:**

1.  **How well does it fit the data we used?** (in-sample fit)
2.  **How well would it predict new data?** (out-of-sample performance)

**These are NOT the same thing!**

------------------------------------------------------------------------

## In-Sample Fit: R²

**R² = 0.208**

"21% of variation in income is explained by population"

**Is this good?**

-   Depends on your goal!
-   For prediction: Moderate
-   For inference: Shows population matters, but other factors exist

**R² alone doesn't tell us if the model is trustworthy**

------------------------------------------------------------------------

## The Problem: Overfitting

**Three scenarios:**

1.  **Underfitting:** Model too simple (high bias)
2.  **Good fit:** Captures pattern without noise
3.  **Overfitting:** Memorizes training data (high variance)

------------------------------------------------------------------------

## Overfitting in Regression

```{r}
#| echo: false
#| eval: true
#| fig-width: 10
#| fig-height: 4

# Create example data with clear pattern
set.seed(123)
x <- seq(0, 10, 0.5)
y <- 2 + 0.5*x + rnorm(length(x), 0, 1)
example_data <- data.frame(x = x, y = y)

# Three models
p1 <- ggplot(example_data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
  labs(title = "Underfitting", subtitle = "Ignores relationship") +
  theme_minimal()

p2 <- ggplot(example_data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Good Fit", subtitle = "Captures true pattern") +
  theme_minimal()

p3 <- ggplot(example_data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ poly(x, 10), se = FALSE) +
  labs(title = "Overfitting", subtitle = "Follows noise") +
  theme_minimal()

library(patchwork)
p1 | p2 | p3
```

**The danger:** High R² doesn't mean good predictions!

------------------------------------------------------------------------

## Train/Test Split

**Solution:** Hold out some data to test predictions

```{r}
#| echo: true
#| eval: true
set.seed(123)
n <- nrow(pa_data)

# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- pa_data[train_indices, ]
test_data <- pa_data[-train_indices, ]

# Fit on training data only
model_train <- lm(median_incomeE ~ total_popE, data = train_data)

# Predict on test data
test_predictions <- predict(model_train, newdata = test_data)
```

------------------------------------------------------------------------

## Evaluate Predictions

```{r}
#| echo: true
#| eval: true
# Calculate prediction error (RMSE)
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma

cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
```

::: callout-note
## Interpreting RMSE

On new data (test set), our predictions are off by \~\$9,500 on average. Is this level of error acceptable for policy decisions?
:::

------------------------------------------------------------------------

## Cross-Validation

**Better approach:** Multiple train/test splits ![](images/cv-illustration.svg){width="80%"}

------------------------------------------------------------------------

## Cross-Validation in Action {.smaller}

```{r}
#| echo: true
#| eval: true
library(caret)

# 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

cv_model <- train(median_incomeE ~ total_popE,
                  data = pa_data,
                  method = "lm",
                  trControl = train_control)

cv_model$results
```

::: {style="font-size: 0.6em;"}
**Key Outputs (averaged across 10 Folds)**

**Root Mean Square Error**

-   The main metric for prediction accuracy
-   On average, across all 10 folds, predictions are off by \~\$12,578
-   Lower is better of course!
-   Most important number for prediction quality (larger errors count more)

**R-Squared**

-   \% of income variation explained by population

**MAE**

-   Mean absolute error: the average absolute prediction error
-   More interpretable than RMSE (no squaring)
:::

------------------------------------------------------------------------

# Part 5: Checking Assumptions

## When Can We Trust This Model?

Linear regression makes assumptions. If violated:

-   Coefficients may be biased
-   Standard errors wrong
-   Predictions unreliable

**We must check diagnostics** before trusting any model

------------------------------------------------------------------------

## Assumption 1: Linearity

**What we assume:** Relationship is actually linear

**How to check:** Residual plot

```{r}
#| echo: true
#| eval: true
pa_data$residuals <- residuals(model1)
pa_data$fitted <- fitted(model1)

ggplot(pa_data, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals") +
  theme_minimal()
```

---

## Reading Residual Plots

::::: columns
::: {.column width="50%"}
**Good**

-   Random scatter
-   Points around 0
-   Constant spread
:::

::: {.column width="50%"}
**Bad**

-   Curved pattern
-   Model missing something
-   Predictions biased
:::
:::::

::: callout-important
## Why This Matters for Prediction

**Linearity violations hurt predictions, not just inference:**

-   If the true relationship is curved and you fit a straight line, you'll systematically underpredict in some regions and overpredict in others
-   **Biased predictions** in predictable ways (not random errors!)
-   Residual plots should show **random scatter** - any pattern means your model is missing something systematic
:::

---
## Assumption 2: Constant Variance

**Heteroscedasticity:** Variance changes across X

**Impact:** Standard errors are wrong → p-values misleading

Look for: "Funnel" or "megaphone" shape in residual plot = heteroscedasticity

::: callout-warning
## What Heteroskedasticity Tells You

**Often a symptom of model misspecification:**

-   Model fits well for some values (e.g., small counties) but poorly for others (large counties)
-   May indicate **missing variables** that matter more at certain X values
-   Ask: "What's different about observations with large residuals?"

**Example:** Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.
:::

---

## Heteroskedasticity visualized
![](images/heteroskedasticity-visual.svg)
*Often a Symptom of Missing Variables*
If variance increases with X, ask: "What differs about high-variance observations?" 
This often reveals important omitted predictors.


---
## Formal Test

```{r}
#| echo: true
#| eval: true
library(lmtest)
bptest(model1)
```


If Heteroscedasticity Detected:

**Solutions:**

1.  **Transform Y:** Try `log(income)`
2.  **Robust standard errors:** `coeftest(model1, vcov = vcovHC)`
3.  **Accept it:** Point predictions still OK for prediction-only goals

---

## Assumption 3: No Multicollinearity

**For multiple regression:** Predictors shouldn't be too correlated

```{r}
#| echo: true
#| eval: false
library(car)
#vif(model1)  # Variance Inflation Factor

# Rule of thumb: VIF > 10 suggests problems
# Not relevand with only 1 predictor!
```

**Why it matters:** Coefficients become unstable, hard to interpret

---
---

## Assumption 4: No Influential Outliers

**Not all outliers are problems** - only those with high leverage AND large residuals

::: {.columns}
::: {.column width="50%"}
### Visual Diagnostic

```{r}
#| echo: true
#| eval: true
#| fig-width: 5
#| fig-height: 4

# Add diagnostic measures
pa_data <- pa_data %>%
  mutate(
    cooks_d = cooks.distance(model1),
    leverage = hatvalues(model1),
    is_influential = cooks_d > 4/nrow(pa_data)
  )

# Plot Cook's distance
ggplot(pa_data, aes(x = 1:nrow(pa_data), y = cooks_d)) +
  geom_point(aes(color = is_influential), size = 2) +
  geom_hline(yintercept = 4/nrow(pa_data), 
             linetype = "dashed", color = "red") +
  scale_color_manual(values = c("grey60", "red")) +
  labs(title = "Cook's Distance",
       x = "Observation", y = "Cook's D") +
  theme_minimal() +
  theme(legend.position = "none")
```
:::

::: {.column width="50%"}
### Identify Influential Points

```{r}
#| echo: true
#| eval: true

# Rule of thumb: Cook's D > 4/n
threshold <- 4/nrow(pa_data)

influential <- pa_data %>%
  filter(cooks_d > threshold) %>%
  select(county_name, total_popE, 
         median_incomeE, cooks_d) %>%
  arrange(desc(cooks_d))

influential
```

**Interpretation:**

- **Cook's D > 4/n:** Potentially influential
- High leverage + large residual = pulls regression line
:::
:::

---

## Understanding Influence Components

**Three key measures:**

::: {.columns}
::: {.column width="33%"}
**Leverage**

- How unusual are X values?
- High population county = high leverage
- Position in X-space
:::

::: {.column width="33%"}
**Residual**

- How far from prediction?
- Unusual income given population
- Position in Y-space
:::

::: {.column width="33%"}
**Influence (Cook's D)**

- Combines both
- Leverage × Residual
- Actually pulls line
:::
:::

```{r}
#| echo: true
#| eval: true
#| fig-width: 10
#| fig-height: 4

# Leverage vs Residual Plot
ggplot(pa_data, aes(x = leverage, y = abs(residuals))) +
  geom_point(aes(size = cooks_d, color = is_influential), alpha = 0.6) +
  geom_hline(yintercept = 2*sd(pa_data$residuals), 
             linetype = "dashed", color = "orange") +
  geom_vline(xintercept = 2*mean(pa_data$leverage), 
             linetype = "dashed", color = "orange") +
  scale_color_manual(values = c("grey60", "red")) +
  labs(title = "Influence Diagnostic: Leverage vs Residual",
       subtitle = "Top-right quadrant = high leverage + large residual (most influential)",
       x = "Leverage", y = "Absolute Residual",
       size = "Cook's D") +
  theme_minimal()
```

---

## What To Do With Influential Points

::: {.callout-tip}
## Investigation Strategy

1. **Investigate:** Why is this observation unusual? 
   - Data entry error?
   - Truly unique case (e.g., Allegheny County)?
   
2. **Report:** Always note influential observations in your analysis

3. **Sensitivity check:** Refit model without them - do conclusions change?

4. **Don't automatically remove:** They might represent real, important cases

**For policy:** An influential county might need **special attention**, not exclusion!
:::

```{r}
#| echo: true
#| eval: false

# Sensitivity analysis
model_without <- lm(median_incomeE ~ total_popE, 
                    data = pa_data %>% filter(!is_influential))

# Compare coefficients
library(broom)
bind_rows(
  tidy(model1) %>% mutate(model = "Full data"),
  tidy(model_without) %>% mutate(model = "Without influential")
)
```

---

## Example: Investigating an Influential County

```{r}
#| echo: true
#| eval: true

# Which county is most influential?
most_influential <- pa_data %>%
  filter(cooks_d == max(cooks_d)) %>%
  select(county_name, total_popE, median_incomeE, 
         cooks_d, leverage, residuals)

most_influential
```

**Questions to ask:**

- Is this a data error? (Check source)
- Is this county truly different? (Urban core? College town?)
- Does removing it change our policy conclusions?
- Should this county get special analysis?

::: {.callout-important}
## Connection to Algorithmic Bias

High-influence observations in demographic data often represent **marginalized communities** or unique populations. Automatically removing them can:

- Erase important populations from analysis
- Make models work worse for underrepresented groups
- Lead to biased policy decisions

**Always investigate before removing!**
:::

---
